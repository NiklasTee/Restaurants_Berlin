{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Place Details\n",
    "\n",
    "Detailed information about a place listed on Google can be accessed through their Places API. Besides address components (country, city, street, latitude, longitude, phone number, etc.), type of business, opening hours and images, **ratings** and **reviews** can be otained as well. The latter is essential for the analysis of the Restaurant dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Place Details API Request\n",
    "\n",
    "Requesting detailed informations about a place requires an API-Key and a 'place_id' like the ones obtained in the previous notebook. Google lets you choose the output parameters and will charge you on that account. The full documentation of all output parameters can be viewed [here](https://developers.google.com/places/web-service/details)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since my 90 days trial was still active, I've decided to go with the default settings and include all availabe output parameters. In addition, there are two availabe output types: json and xml. The built-in Python package, makes json a convenient choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Choosing an 'place_id' for each restaurant\n",
    "\n",
    "Lets have another look at the results of the previous notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from fuzzywuzzy import fuzz\n",
    "import googlemaps\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load modified dataframe\n",
    "data = pd.read_pickle(r'data/restaurants_with_google_id.pkl')\n",
    "df = pd.DataFrame(data, columns=[\n",
    "    'name',\n",
    "    'fon_place_id',\n",
    "    'name_place_id',\n",
    "])\n",
    "\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that we used two approaches receiving the ids. Therefore, some restaurants have multiple entries while others have no entry at all. Since it is not possible to tell which id is the right one for each place, I've decided to run API requests for all available ids and match the output afterwards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating a list with all available ids\n",
    "place_ids = []\n",
    "\n",
    "for ids in df['fon_place_id']:\n",
    "    id_list = ast.literal_eval(ids)  # dissolve nested list in dataframe\n",
    "    for id in id_list:\n",
    "        if id in place_ids:  # list shall only contain unique ids\n",
    "            pass\n",
    "        else:\n",
    "            place_ids.append(id)\n",
    "\n",
    "for ids in df['name_place_id']:\n",
    "    id_list = ast.literal_eval(ids)  # dissolve nested list in dataframe\n",
    "    for id in id_list:\n",
    "        if id in place_ids:  # list shall only contain unique ids\n",
    "            pass\n",
    "        else:\n",
    "            place_ids.append(id)\n",
    "            \n",
    "print(\"list of ids: \")\n",
    "print(str(len(place_ids)) + \" entries\")\n",
    "print(\"\\n\")\n",
    "print(place_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Requesting Place Details\n",
    "\n",
    "Now that we have decided on the input, the API request can be specified:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API specification\n",
    "api_key = input(\"Enter your API-Key here: \")\n",
    "gmaps = googlemaps.Client(key=api_key)\n",
    "\n",
    "# Executing the API request\n",
    "for id in place_ids:\n",
    "        places_result = gmaps.place(place_id=id)  # output parameters can be specified here - default used\n",
    "        with open(f'place_details/{id}.json', 'w') as outfile:  # file name = place_id\n",
    "            json.dump(places_result, outfile)  # json package for saving each output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the request I received **812 json files**. Therefore, all requests were successful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what is the output looking like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('data/ChIJow5SxuxRqEcRpk680jewg50.json', 'r') as infile:\n",
    "    example = json.load(infile)\n",
    "print(json.dumps(example, indent=5, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>As mentioned above, the file contains detailed address ('address_components', 'geometry', 'international_phone_number') and business ('name\", 'types', 'business_status',  'permanently_closed', 'website', 'photos') informations. Of interest for the analysis are the **'rating'** and **'user_ratings_total'** specifications. Furthermore, up to five user reviews are included."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging API Results and Restaurants Data\n",
    "\n",
    "The data from the 812 json files shall be appended to the original Restaurants dataset. We make use of the fact, that each json file is named by its corresponding 'place_id'. Json files are structured in dictionary-like way, making it easy to access each element of the file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Structuring the JSON output\n",
    "\n",
    "We start by creating a pandas dataframe with all relevant output from the API request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating empty dataframe with relevant output for all 'place_id's\n",
    "details_df = pd.DataFrame(index=place_ids, columns=['name',\n",
    "                                                    'city',\n",
    "                                                    'bezirk',\n",
    "                                                    'street_nr',\n",
    "                                                    'lat',\n",
    "                                                    'lng',\n",
    "                                                    'types',\n",
    "                                                    'business_status',\n",
    "                                                    'price_level',\n",
    "                                                    'rating',\n",
    "                                                    'user_ratings_total'])\n",
    "\n",
    "# Fill dataframe with informations by iterating over json files\n",
    "for id in place_ids:\n",
    "    try:\n",
    "        f = open(f'place_details/{id}.json', 'r')  # there is a corresponding json file for each id \n",
    "        data = json.loads(f.read())\n",
    "        \n",
    "        # add name\n",
    "        name = data['result']['name']  # path in dictionary / json file\n",
    "        details_df.loc[id, 'name'] = name\n",
    "        # add city\n",
    "        city = data['result']['address_components'][3]['long_name']\n",
    "        details_df.loc[id, 'city'] = city\n",
    "        # add bezirk\n",
    "        bezirk = data['result']['address_components'][2]['short_name']\n",
    "        details_df.loc[id, 'bezirk'] = bezirk\n",
    "        # add street_nr\n",
    "        street = data['result']['address_components'][1]['long_name']\n",
    "        number = data['result']['address_components'][0]['short_name']\n",
    "        street_nr = street + \" \" + number\n",
    "        details_df.loc[id, 'street_nr'] = street_nr\n",
    "        # add lat\n",
    "        lat = data['result']['geometry']['location']['lat']\n",
    "        details_df.loc[id, 'lat'] = lat\n",
    "        # add lng\n",
    "        lng = data['result']['geometry']['location']['lng']\n",
    "        details_df.loc[id, 'lng'] = lng\n",
    "        # add types\n",
    "        types = data['result']['types']\n",
    "        details_df.loc[id, 'types'] = types\n",
    "         # add business_status\n",
    "        business_status = data['result']['business_status']\n",
    "        details_df.loc[id, 'business_status'] = business_status\n",
    "        # add price_level\n",
    "        price_level = data['result']['price_level']\n",
    "        details_df.loc[id, 'price_level'] = price_level\n",
    "        # add rating\n",
    "        rating = data['result']['rating']\n",
    "        details_df.loc[id, 'rating'] = rating\n",
    "        # add user_ratings\n",
    "        user_ratings_total = data['result']['user_ratings_total']\n",
    "        details_df.loc[id, 'user_ratings_total'] = user_ratings_total\n",
    " \n",
    "    except KeyError:  # price_level isn't specified for all places\n",
    "        try:\n",
    "            # add rating\n",
    "            rating = data['result']['rating']\n",
    "            details_df.loc[id, 'rating'] = rating\n",
    "            # add num_ratings\n",
    "            user_ratings_total = data['result']['user_ratings_total']\n",
    "            details_df.loc[id, 'user_ratings_total'] = user_ratings_total\n",
    "        except KeyError: # when there is no 'rating' there is no need to end the loop since 'user_ratings_total' will be null as well\n",
    "            pass\n",
    "    except IndexError:  # in case some API requests were not sucessful\n",
    "        continue\n",
    "\n",
    "# Replacing NaN with zero\n",
    "details_df['user_ratings_total'].fillna(0, inplace=True)\n",
    "\n",
    "display(details_df)\n",
    "\n",
    "# Saving results\n",
    "details_df.to_pickle(r'data/details_df.pkl')\n",
    "details_df.to_csv(r'data/details_df.csv', sep=';', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> Each row is now indexed with the 'place_id' and all column information origins from the API request."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Merging Dataframes\n",
    "\n",
    "The Restaurants dataframe from previous notebook has 720 entries. The API results dataframe has 812 entries. Since there is a surplus of data to match, we need to select the right API results to append to the Restaurant dataframe. The 'place_id' seems to be a convenient matching key, since it is represented in both dataframes. However, as already mentioned some places have multiple ids. In that case, other matching criterias are needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by reading both dataframes. Since the ids within the Restaurants dataframe are not recognized as lists but as a sequence of strings, we need to convert them using the 'ast' module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataframes\n",
    "data = pd.read_pickle(r'data/restaurants_with_google_id.pkl')\n",
    "restaurants_df = pd.DataFrame(data)\n",
    "\n",
    "data = pd.read_pickle(r'data/details_df.pkl')\n",
    "details_df = pd.DataFrame(data)\n",
    "\n",
    "# Convert string with ids to list\n",
    "restaurants_df.insert(loc=8, column='clean_name_place_id', value=None)\n",
    "for idx, ids in enumerate(restaurants_df['name_place_id']):\n",
    "    clean_ids = ast.literal_eval(ids)\n",
    "    restaurants_df.at[idx, 'clean_name_place_id']=clean_ids\n",
    "\n",
    "restaurants_df.insert(loc=8, column='clean_fon_place_id', value=None)\n",
    "for idx, ids in enumerate(restaurants_df['fon_place_id']):\n",
    "    clean_ids = ast.literal_eval(ids)\n",
    "    restaurants_df.at[idx, 'clean_fon_place_id']=clean_ids\n",
    "\n",
    "# Create list with unique ids for each restaurant\n",
    "restaurants_df['place_ids'] = restaurants_df['clean_fon_place_id']+restaurants_df['clean_name_place_id']  # combine place ids\n",
    "restaurants_df.insert(loc=11, column='clean_place_ids', value=None)  # create new column\n",
    "restaurants_df['clean_place_ids'] = restaurants_df['clean_place_ids'].astype('object')  # set type\n",
    "for idx, ids in enumerate(restaurants_df['place_ids']):\n",
    "    unique_ids = list(set(ids))  # remove duplicates\n",
    "    restaurants_df.at[idx, 'clean_place_ids'] = unique_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataframes to be merged:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(restaurants_df)\n",
    "display(details_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The informations from the API requests shall now be appended to the Restaurants dataframe. We start by iterating over the Restaurants dataframe. Within each row, there are zero, one or multiple place ids. When there is only one entry, we can simply look up the id in the details dataframe and append the corresponding informations. For multiple ids, a benchmark on choosing the right API result is needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataframe for appending place details\n",
    "restaurants_df.insert(loc=12, column='place_id', value=None)\n",
    "restaurants_df.insert(loc=13, column='bezirk', value=None)\n",
    "restaurants_df.insert(loc=14, column='lat', value=None)\n",
    "restaurants_df.insert(loc=15, column='lng', value=None)\n",
    "restaurants_df.insert(loc=16, column='types', value=None)\n",
    "restaurants_df.insert(loc=17, column='price_level', value=None)\n",
    "restaurants_df.insert(loc=18, column='rating', value=None)\n",
    "restaurants_df.insert(loc=19, column='user_ratings_total', value=None)\n",
    "\n",
    "# Initialize criteria lists on which the API results are matched\n",
    "benchmark_name = []\n",
    "benchmark_address = []\n",
    "benchmark_number_of_ratings = []\n",
    "benchmark_types = []\n",
    "benchmark_city = []\n",
    "\n",
    "# Appending place details\n",
    "for index, restaurant in restaurants_df.iterrows():\n",
    "    for id in restaurant['clean_place_ids']:\n",
    "        \n",
    "        # name benchmark\n",
    "        name1 = restaurant['name']\n",
    "        name2 = details_df.loc[id, 'name']\n",
    "        try:\n",
    "            levenshtein_distance_name = fuzz.ratio(name1.lower(), name2.lower())  # calculate Levenshtein distance\n",
    "        except AttributeError: # names with integers cannot be lowered\n",
    "            try:\n",
    "                levenshtein_distance_name = fuzz.ratio(name1, name2)\n",
    "            except TypeError:  # check for missing values (NaN = integer)\n",
    "                levenshtein_distance_name = 0\n",
    "        except TypeError:  # check for missing values (NaN = integer)\n",
    "            levenshtein_distance_name = 0\n",
    "        benchmark_name.append(levenshtein_distance_name)\n",
    "        \n",
    "        # address benchmark\n",
    "        address1 = restaurant['strasse_nr']\n",
    "        address2 = details_df.loc[id, 'street_nr']\n",
    "        try:\n",
    "            levenshtein_distance_address = fuzz.ratio(address1, address2)\n",
    "        except TypeError:  # check for missing values (NaN = integer)\n",
    "            levenshtein_distance_address = 0\n",
    "        benchmark_address.append(levenshtein_distance_address)\n",
    "       \n",
    "        # user ratings benchmark\n",
    "        user_ratings_total = details_df.loc[id, 'user_ratings_total']\n",
    "        benchmark_number_of_ratings.append(user_ratings_total)\n",
    "        \n",
    "        # types benchmark\n",
    "        types = details_df.loc[id, 'types']\n",
    "        try:\n",
    "            if 'restaurant' in types:\n",
    "                type_score = 1\n",
    "            elif 'food' in types:\n",
    "                type_score = 0.75\n",
    "            elif 'cafe' in types:\n",
    "                type_score = 0.75\n",
    "            elif 'bar' in types:\n",
    "                type_score = 0.5\n",
    "            else:\n",
    "                type_score = 0\n",
    "        except TypeError:  # check for missing values (NaN = integer)\n",
    "            type_score = 0\n",
    "        benchmark_types.append(type_score)\n",
    "        \n",
    "        # city benchmark\n",
    "        city = details_df.loc[id, 'city']\n",
    "        try:\n",
    "            if city == \"Berlin\":\n",
    "                city_score = 1\n",
    "            elif city == \"Germany\":\n",
    "                city_score = 0.5\n",
    "            else:\n",
    "                city_score = 0\n",
    "        except TypeError:  # check for missing values (NaN = integer)\n",
    "            city_score = 0\n",
    "        benchmark_city.append(city_score)\n",
    "\n",
    "    # Choosing the right place_id\n",
    "    bname = np.array(benchmark_name)\n",
    "    baddress = np.array(benchmark_address)\n",
    "    bnumber_of_ratings = np.array(benchmark_number_of_ratings)\n",
    "    btypes = np.array(benchmark_types)\n",
    "    bcity = np.array(benchmark_city)\n",
    "\n",
    "    benchmark = (bname + baddress/2 + bnumber_of_ratings/10)*btypes*bcity\n",
    "    try:\n",
    "        highest_ranked = np.argmax(benchmark)\n",
    "        best_id = restaurant['clean_place_ids'][highest_ranked]\n",
    "    except ValueError:  # some places are without id\n",
    "        best_id = None\n",
    "\n",
    "    # Appending information to restaurant dataframe\n",
    "    try:\n",
    "        restaurants_df.at[index, 'place_id'] = best_id\n",
    "        restaurants_df.at[index, 'bezirk'] = details_df.loc[best_id, 'bezirk']\n",
    "        restaurants_df.at[index, 'lat'] = details_df.loc[best_id, 'lat']\n",
    "        restaurants_df.at[index, 'lng'] = details_df.loc[best_id, 'lng']\n",
    "        restaurants_df.at[index, 'types'] = details_df.loc[best_id, 'types']\n",
    "        restaurants_df.at[index, 'price_level'] = details_df.loc[best_id, 'price_level']\n",
    "        restaurants_df.at[index, 'rating'] = details_df.loc[best_id, 'rating']\n",
    "        restaurants_df.at[index, 'user_ratings_total'] = details_df.loc[best_id, 'user_ratings_total']\n",
    "    except KeyError:  # some values are missing\n",
    "        continue\n",
    "\n",
    "    # Prepare next loop\n",
    "    del benchmark_name[:]\n",
    "    del benchmark_address[:]\n",
    "    del benchmark_number_of_ratings[:]\n",
    "    del benchmark_types[:]\n",
    "    del benchmark_city[:]\n",
    "\n",
    "# Delete redundant columns\n",
    "restaurants_df.drop(\"fon_place_id\", axis=1, inplace=True)\n",
    "restaurants_df.drop(\"clean_fon_place_id\", axis=1, inplace=True)\n",
    "restaurants_df.drop(\"name_place_id\", axis=1, inplace=True)\n",
    "restaurants_df.drop(\"clean_name_place_id\", axis=1, inplace=True)\n",
    "restaurants_df.drop(\"place_ids\", axis=1, inplace=True)\n",
    "restaurants_df.drop(\"clean_place_ids\", axis=1, inplace=True)\n",
    "restaurants_df.drop(\"name_street\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets have a closer look at the benchmarks that decided which output to use, when there where multiple ids:\n",
    "\n",
    "*1. Name Benchmark*\n",
    "<br>Uses Levenshtein distance to compare the name of the restaurant in the original dataframe and in the API output.\n",
    "\n",
    "*2. Address Benchmark*\n",
    "<br>Uses Levenshtein distance to compare the address of the restaurant in the original dataframe and in the API output.\n",
    "\n",
    "*3. User ratings Benchmark*\n",
    "<br>Places with more reviews are more likely to be restaurants.\n",
    "\n",
    "*4. Types Benchmark*\n",
    "<br>Checks if the place is a restaurant, something similar or totally different.\n",
    "\n",
    "*5. City Benchmark*\n",
    "<br>Checks if the place is loacted in Berlin.\n",
    "\n",
    "<br> Based on the *Total Benchmark* that is created of all five benchmarks, the most promising 'place_id' output is choosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\">>> merged dataframe <<<\")\n",
    "display(restaurants_df)\n",
    "\n",
    "# Saving results\n",
    "restaurants_df.to_pickle(r'data/detailed_restaurants.pkl')\n",
    "restaurants_df.to_csv(r'data/detailed_restaurants.csv', sep=';', encoding='utf-8', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>Now the information of the place details API request is appended to the Restaurants data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
