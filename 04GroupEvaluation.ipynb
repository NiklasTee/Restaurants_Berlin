{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Groups of Restaurants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Picking up on the noticed correlation of number of ratings and rating, we want to further examine possible natures of this connection. As mentioned in the previous notebook, an omitted variable, that is correlated with the 'nr_ratings' parameter and affects the 'rating' parameter, might bias the regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One could think that the type of a restaurant must be decisive over many outcomes. Some restaurants for instance are trying to serve the highest possible amount of costumers, while others focus on quality and ambient. Sushi tends to be more expensive than burgers. Italian restaurants are frequented during dinner time while Fast Food peaks at lunch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to further examine this phenomenon and therefore try to cluster the restaurants in our data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to form a group, senseful categories are needed. The original csv file contains a column 'angebot' which specifies the type of food a restaurant is offering. Lets load the dataframe from previous notebook and append the 'angebot' information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import distance\n",
    "from fuzzywuzzy import fuzz\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, seaborn as sns\n",
    "from sklearn import cluster\n",
    "from sklearn import linear_model\n",
    "\n",
    "# Load dataframe from previous notebook\n",
    "data = pd.read_pickle(r'data/modif_restaurants.pkl')\n",
    "modif_df = pd.DataFrame(data)\n",
    "\n",
    "# Load original csv file\n",
    "data2 = pd.read_csv(r'data/restaurants.csv',  delimiter=';')\n",
    "restaurants_df = pd.DataFrame(data2, columns=[\n",
    "    'unique_id',\n",
    "    'angebot',])\n",
    "\n",
    "# Merge dataframes\n",
    "restaurants_df.set_index('unique_id', inplace=True)  # prepare for merging\n",
    "df = pd.concat([modif_df, restaurants_df], axis=1, join='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can examine, what the most rated restaurants are offering that makes them so popular:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Create new dataframe with restaurants rated more than 1000 times\n",
    "often_rated_restaurants = df.drop(df[df.nr_ratings < 1000].index, inplace=False)  # filter dataframe\n",
    "often_rated_restaurants.sort_values(by=['nr_ratings'], inplace=True, ascending=False)  # sort\n",
    "count_series = often_rated_restaurants.name.str.count(\"McDonald\")  # count in dataframe\n",
    "\n",
    "display(often_rated_restaurants[['name', 'nr_ratings', 'price_lvl', 'angebot']])\n",
    "print(often_rated_restaurants['price_lvl'].value_counts())\n",
    "print(\"\\n\")\n",
    "print(\"Number of McDonald's Franchises: \" + str(count_series.sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority of restaurants with more than 1000 reviews are located in the \"Moderate\" price level. While this is coherent with the total sample average, the distribution of the conditional dataframe still deviates from the expected distribution. Especially, places without price category are underepresented. Also worth mentioning is, that there is only one \"Expensive\" restaurant in the selection. We already discussed possible reasons for both phenomenons in previous notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we take a look at each restaurants name and 'angebot', it stands out that multiple franchise restaurants (e.g. 6 times McDonald's) are represented in the selection. This will be the first approach for creating groups (see **Franchise vs Privately Owned**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can also identify a variety of food types offered. While Burger, Pizza and Pasta seem to thrive, traditional German food is represented as well. We will have a closer look at this distribution within the second clustering approach (see **Type of Food**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Franchise vs Privately Owned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some franchises in the selected dataframe are recognized quickly. We can create a list of those. Additionally, all restaurants that occur more than one time in the dataframe shall be appended to the list: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty list that will hold unique ids of all franchise restaurants\n",
    "franchise_ids = []\n",
    "\n",
    "# Create list with known franchises\n",
    "known_franchises = [\"McDonald\", \"Subway\", \"Osteria\", \"Bonanza Coffee\", \"Dominos\", \"Hard Rock Cafe\", \"BLOCK HOUSE\",\n",
    "                    \"Jim Block\", \"Dolores\"]\n",
    "\n",
    "# Detect multiple entries in dataframe\n",
    "name_distribution = df['name'].value_counts()\n",
    "for name, count in name_distribution.iteritems():\n",
    "    if count > 1:\n",
    "        known_franchises.append(name)  # append name to list\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "# Iterate over list wih known franchises\n",
    "for index, restaurant in df.iterrows():\n",
    "    for franchise in known_franchises:\n",
    "        if franchise in restaurant['name']:\n",
    "            franchise_ids.append(index)  # append unique id as matching criteria to list\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "# Remove duplicates\n",
    "franchise_ids = list(set(franchise_ids))\n",
    "\n",
    "print(\"Number of Franchise Restaurants: \" + str(len(franchise_ids)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we gained new information about the restaurants we can expand the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append Franchise category as column to dataframe\n",
    "df.insert(loc=7, column='franchise', value=int)\n",
    "\n",
    "# Fill column with booleans\n",
    "for index, restaurant in df.iterrows():\n",
    "    if index in franchise_ids:\n",
    "        df.at[index, 'franchise'] = 1\n",
    "    else:\n",
    "        df.at[index, 'franchise'] = 0\n",
    "\n",
    "# Convert type to bool\n",
    "df['franchise'] = df['franchise'].astype('bool')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The information is stored as a boolean variable - 1 if Franchise, 0 if not. Are there any deviations within the groups from the sample average?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Create dataframe with group means\n",
    "means = df.groupby('franchise')[['rating', 'nr_ratings']].mean().round(decimals=2)\n",
    "means = means.rename(columns={'rating': \"mean_rating\", 'nr_ratings': \"mean_nr_ratings\"})\n",
    "\n",
    "# Create dataframe with group standard deviations\n",
    "stds = df.groupby('franchise')[['rating', 'nr_ratings']].std().round(decimals=2)\n",
    "stds = stds.rename(columns={'rating': \"std_rating\", 'nr_ratings': \"std_nr_ratings\"})\n",
    "\n",
    "# Merge dataframes\n",
    "oview = pd.concat([means, stds], axis=1)\n",
    "\n",
    "# Add mean and standard deviation of total sample as row to output dataframeata\n",
    "describe_df = df.describe()\n",
    "mean_rating = describe_df.loc['mean', 'rating']\n",
    "mean_nr_ratings = describe_df.loc['mean', 'nr_ratings']\n",
    "std_rating = describe_df.loc['std', 'rating']\n",
    "std_nr_ratings = describe_df.loc['std', 'nr_ratings']\n",
    "\n",
    "# Count frequency of each row\n",
    "counter_total = describe_df.loc['count', 'rating']\n",
    "counter_series = df['franchise'].value_counts()\n",
    "\n",
    "# Append series as column to output dataframe\n",
    "oview['freq'] = counter_series\n",
    "oview.loc['Total'] = [mean_rating, mean_nr_ratings, std_rating, std_nr_ratings, counter_total]\n",
    "\n",
    "# Format output dataframe\n",
    "oview = oview.astype({'freq': int})\n",
    "oview = oview.round(decimals=2)\n",
    "\n",
    "print(oview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, franchises tend to have more ratings with on average worse ratings than restaurants that are privately owned. Keep in mind that there is likely a bias, since we partly selected the franchises based on the conditional dataframe ('nr_ratings' > 1000). Lets know examine, if the negative correlation of 'nr_ratings' on 'rating' discussed in previous notebook holds:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multivariate Linear Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new dataframe exclusiv outlier\n",
    "small_restaurants_df = df.drop(df[df.nr_ratings > 5000].index, inplace=False)\n",
    "\n",
    "# Multivariate linear regression of number of ratings and franchise on rating\n",
    "X = small_restaurants_df[['nr_ratings', 'franchise']]\n",
    "y = small_restaurants_df['rating']\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(X, y)\n",
    "\n",
    "print(\"Intercept: \\n\", regr.intercept_)\n",
    "print(\"Coefficients: \")\n",
    "for coef in regr.coef_:\n",
    "    print(format(coef, 'f'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficients still indicate a negative correlation of 'nr_ratings' and 'rating'. However, when controlling for the restaurant type the amount almost halved. This happens because the number of ratings is correlated to the type of restaurant (i.e. 'franchise') as well. The latter affects a restaurants rating by 0.3545 stars on average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3D Plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "# Create 3d scatter plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Variables\n",
    "X = small_restaurants_df['nr_ratings']\n",
    "Y = small_restaurants_df['franchise']\n",
    "z = small_restaurants_df['rating']\n",
    "\n",
    "# Plot points\n",
    "ax.scatter(X, Y, z)\n",
    "cmap = ListedColormap(sns.color_palette(\"husl\", 2).as_hex())  # get colors\n",
    "sc = ax.scatter(X, Y, z, s=40, c=Y, marker='o', cmap=cmap, alpha=1)\n",
    "\n",
    "# Labeling\n",
    "ax.set_xlabel('nr_ratings')\n",
    "ax.set_ylabel('franchise')\n",
    "ax.set_zlabel('rating')\n",
    "\n",
    "# Produce a legend\n",
    "L = plt.legend(*sc.legend_elements(), bbox_to_anchor=(1.05, 1), loc=1, title=\"franchise\")\n",
    "L.get_texts()[0].set_text('False')\n",
    "L.get_texts()[1].set_text('True')   \n",
    "              \n",
    "# Rotate the axes\n",
    "for angle in range(0, 360):\n",
    "    ax.view_init(30, angle)\n",
    "    plt.draw()\n",
    "    plt.pause(.001)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Type of Food"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no common approach for grouping restaurants. Obviously assigning every food type to their origin region would be convenient. Yet there are a lot of crossover and nationality-independent restaurants especially in a multicultural city like Berlin. We therefore need to think of more diverse categories - or let an algorithm do just that for us:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the knowledge of the restaurant 'angebot' data, one can create sensful categories to match the restaurants with afterwards. The following code is mainly based on string comparision using the Levenshtein distance and substring characteristics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list with known food types\n",
    "food_types = [\"französisch\", \"asiatisch\", \"deutsch\", \"italienisch\", \"russisch\", \"griechisch\", \"thai\", \"mexikanisch\", \"indisch\", \"japanisch\", \"chinesisch\", \"koreanisch\", \"international\", \"arabisch\", \"orientalisch\",\n",
    "              \"suppe\", \"burger\", \"döner\", \"sushi\", \"kaffee\", \"kuchen\", \"steak\", \"pizza\", \"pasta\", \"salat\", \"bbq\", \"bowl\",\n",
    "              \"regional\", \"vegan\", \"vegetarisch\", \"saisonal\"]\n",
    "\n",
    "# Append new column to dataframe\n",
    "df.insert(loc=8, column='food_type', value=None)\n",
    "\n",
    "# Check for food type in dataframe\n",
    "for index, restaurant in df.iterrows():\n",
    "    f_types = []\n",
    "    angebot = restaurant['angebot']\n",
    "    angebot_list = angebot.split()\n",
    "    for word in angebot_list:\n",
    "        word = word.lower()\n",
    "        for type in food_types:\n",
    "            levenshtein_distance = fuzz.ratio(type, word)\n",
    "            if type in word and type not in f_types:\n",
    "                f_types.append(type)\n",
    "            elif levenshtein_distance > 85 and type not in f_types:\n",
    "               f_types.append(type)\n",
    "            else:\n",
    "                pass\n",
    "    df.at[index, 'food_type'] = f_types\n",
    "    \n",
    "display(df[['name', 'angebot', 'food_type']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Levenshtein distance and the substring function perform quite good at identifying the prior specified food types within the 'angebot' column. There are problems though, when the offer is described in any other language than German. Hence, the following output should be evaluated critically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Set new index to match array\n",
    "new_indexed_df = df.reset_index()\n",
    "\n",
    "# Create array with matching index\n",
    "col_array = new_indexed_df[\"food_type\"].to_numpy(na_value=None)\n",
    "\n",
    "# Create and prepare dataframe to display output in\n",
    "eval_df = pd.DataFrame({'food_type': food_types,\n",
    "                        'mean_rating': None,\n",
    "                        'mean_nr_ratings': None,\n",
    "                        'std_rating': None,\n",
    "                        'std_nr_ratings': None,\n",
    "                        'frequency': None,\n",
    "                        })\n",
    "\n",
    "eval_df.set_index('food_type', inplace=True)  # set index\n",
    "\n",
    "# Creating mask for calculating conditional benchmarks\n",
    "for food_type in food_types:  # loop through prior specified list\n",
    "    mask = []  # set up list\n",
    "    for restaurant in col_array:\n",
    "        cond = food_type in restaurant\n",
    "        mask.append(cond)  # fill list with True/False\n",
    "    mask_array = np.array(mask) # list to array\n",
    "    food_type_df = new_indexed_df[mask_array]  # fancy indexing with mask array\n",
    "    describe_df = food_type_df.describe()  # calculate output\n",
    "    mean_rating = describe_df.loc['mean', 'rating']\n",
    "    mean_nr_ratings = describe_df.loc['mean', 'nr_ratings']\n",
    "    std_rating = describe_df.loc['std', 'rating']\n",
    "    std_nr_ratings = describe_df.loc['std', 'nr_ratings']\n",
    "    freq = describe_df.loc['count', 'rating']\n",
    "    eval_df.at[food_type, 'mean_rating'] = mean_rating\n",
    "    eval_df.at[food_type, 'mean_nr_ratings'] = mean_nr_ratings\n",
    "    eval_df.at[food_type, 'std_rating'] = std_rating\n",
    "    eval_df.at[food_type, 'std_nr_ratings'] = std_nr_ratings\n",
    "    eval_df.at[food_type, 'frequency'] = freq\n",
    "\n",
    "# Sort by frequency descending\n",
    "eval_df.sort_values(by=['frequency'], axis=0, ascending=False, inplace=True)\n",
    "eval_df['frequency'] = eval_df['frequency'].astype(int)\n",
    "print(eval_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some remarkable differences among the restaurant types. If we take for example Italian food which is in general rather expensive and compare it to restaurants offering Burgers, we see that the latter receive on average almost three times as many ratings. This might be due to higher customer quantity. The average Italian restaurant however, scores better ratings (~0.2 stars). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datframe with both the franchise and food type information shall be used for the geographical visualization with kepler later on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r'data/geo_restaurants.csv', sep=',', encoding='utf-8', index=True)\n",
    "df.to_pickle(r'data/geo_restaurants.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clustering Algorithm - DBSCAN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What makes DBSCAN a convinient choice for clustering the food offers in the Restaurant dataset, is its non-parametric property. The number of clusters does not need to be specified in advance. Therefore, the clustering result only depends on epsilon (i.e. radius of the neighborhood) and the minPts (i.e. minimum number of points within neighborhood to be classified as core point). In addition, the algorithm detects outliers as noise points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The description within the 'angebot' column of each restaurant is split into words of which each one is appended to a list. Since the entries  vary a lot - some places are described in sentences others in bullet points - we first need to remove meaningless fillwords. Fortunately, lists with fillwords exist for most languages. In addition, punctuation marks and other meaningless characters need to be striped:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list with words in 'angebot' column\n",
    "angebot_words = []\n",
    "for index, restaurant in df.iterrows():  # iterate over dataframe rows\n",
    "    list_words = restaurant['angebot'].split()  # split string into words\n",
    "    for word in list_words:\n",
    "        angebot_words.append(word.lower())  # no capitalization wanted\n",
    "print(\"Length raw list: \" + str(len(angebot_words)))\n",
    "\n",
    "# Load list with German fillwords - source: https://gist.github.com/makelefy/f5ff045c08f8982d130ccd4c5b616019\n",
    "fill_words = []\n",
    "with open (\"data/stopWords.txt\") as file_object:\n",
    "    for line in file_object:\n",
    "        fillword = line.replace(\"\\n\", \"\")\n",
    "        fill_words.append(fillword)\n",
    "\n",
    "# Create new list without fillwords\n",
    "raw_food_words = [word for word in angebot_words if word not in fill_words]  # append if not fillword\n",
    "food_words = []\n",
    "for raw_word in raw_food_words:\n",
    "    clean_word = re.sub('[^a-zäüöß]+', '', raw_word)  # clean strip\n",
    "    food_words.append(clean_word)\n",
    "\n",
    "print(\"Length cleansed list: \" + str(len(food_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>Grupping unlabeled data requires information about the similarity of the instances. Since we aim on clustering strings, common distances like the euclidean norm cannot be used. One of the most appropriate functions for string comparison is the Levensthein distance. It measures the difference between two strings by counting the number of necessary character edits to convert one string into the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are over 3500 instances in our list. A square matrix with the distance between each point and in both ways has over 12 million entries. Calculating the matrix took my system (AMD Ryzen 7 4700U, 16GB RAM, Winx64) about 15 minutes. You will find the corresponding code in the cell below. The size of the  csv file is around 300MB and cannot be uploaded to GitHub (100MB limit). If you do not want to run the cell, then I suggest you download the file [here](https://www.dropbox.com/sh/p8aq23l99jwezw1/AAChdIf7zpL0eiyVFlyXeEyta?dl=0). Make sure that you move the file to the 'data' folder afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Levensthein distance metrix !!!long runtime!!!\n",
    "words = np.asarray(food_words)  # list to array\n",
    "lev_metric = np.array([[distance.levenshtein(w1,w2) for w1 in words] for w2 in words])  # create distance matrix\n",
    "np.savetxt(\"data/lev_metric.csv\", lev_metric, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Levensthein distance matrix can now be used to run the DBSCAN algortihm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering\n",
    "lev_metric = np.loadtxt('data/lev_metric.csv', delimiter=\",\")\n",
    "clustering = cluster.dbscan(lev_metric, metric='precomputed', eps=3, min_samples=5)  # run DBSCAN\n",
    "cluster_labels = clustering[1]  # output is tupel of 2 arrays\n",
    "food_words_array = np.array(food_words)  # convert list into array\n",
    "cluster_count=np.max(cluster_labels)+1  # count number of clusters\n",
    "for cluster_number in range(cluster_count):\n",
    "    cluster_x = food_words_array[cluster_labels == cluster_number]  # boolean masking and fancy indexing\n",
    "    print(\"\\n\")\n",
    "    print(\"Cluster \" + str(cluster_number) + \" :\")\n",
    "    print(cluster_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The clustering of round about 3500 instances is done within a few seconds. If you look at the output, you will see that each cluster mainly contains duplicates or words with a little different spelling. Noise points are located in the first cluster. In this application DBSCAN basically counts the frequency of similiar words within the 'angebot' column and groups them in a cluster if their count exceeds the 'min_samples' value. Note that the number and the type of clusters depend heavenly on the specified parameters (i.e. 'eps' & 'min_sample'). I suggest to try tweaking the paramaters yourself in order to see their impact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We specified that there need to be at least four ('min_sample') other instances within a Levensthein distance of three ('eps') for a point to be classified as a core point. When there is a point in reach of a core point but with less than three neighboors, the cluster edge is formed. So, are the clusters created by DBSCAN making sense? We can check this by comparing all restaurants of one exemplary cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose cluster from DBSCAN output with eps=3 and min_samples=5\n",
    "cluster40 = ['thailandische', 'holländische', 'thailändische', 'thailändische','thailändisch', 'thailändische',\n",
    "             'thailändische', 'thailändische', 'thailändisch', 'thailändisch', 'thailändische']\n",
    "\n",
    "# Create list of indexes from restaurants in cluster\n",
    "restaurants_in_cluster40 = []\n",
    "for idx, restaurant in df.iterrows():\n",
    "    for word in cluster40:\n",
    "        if word in restaurant['angebot'] and idx not in restaurants_in_cluster40:\n",
    "            restaurants_in_cluster40.append(idx)\n",
    "\n",
    "# Select dataframe columns from list\n",
    "cluster40_df = df.loc[restaurants_in_cluster40]  # indexing with list\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "display(cluster40_df[[\"name\", \"delivery\", \"franchise\", \"price_lvl\", \"nr_ratings\", \"rating\", \"food_type\", \"angebot\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the 'angebot' column of the dataframe, we see that 4 out of 5 restaurants are actually serving Thai food. Those were already recognized by our manual clustering efforts. The one outlier is serving dutch fries which translate to 'holländische Fritten'. The Levensthein distance  between the latter and 'thailändische' is 3. Therefore, it is considered a core point and thus part of the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that the DBSCAN algorithm performs quite good at detecting similiar strings. Unfortunately, it is not able to group the  food types in sensful categories. Some categories (e.g. 'produkte') have none or little meaning. Furthermore, some categories that humans might group together (e.g. 'burger' and 'american') cannot be recognized by DBSCAN. Since the results of the manual clustering seem to be more appropriate, we won't append the algorithm results to the Restaurants dataframe."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
