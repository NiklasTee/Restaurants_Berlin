{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modified Dataset\n",
    "\n",
    "The purpose of this notebook is to get an overview on the expanded Restaurants data. Especially the parameters extracted from the Google API are of interest. An elementary understanding of the parameters is key to comprehend the more specific analyzes later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Dataframe\n",
    "\n",
    "A few refactoring steps will make the data more clear. We start by loading the merged dataframe from previous notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_pickle(r'data/detailed_restaurants.pkl')\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some columns are not of interest in our analyzes and can be dropped. Additionally, we set a new index, change column names and reorder just these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Drop columns not of interest\n",
    "df.drop(\"art\", axis=1, inplace=True)  # all equal \"Gastronomie\"\n",
    "df.drop(\"fon\", axis=1, inplace=True)\n",
    "df.drop(\"place_id\", axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Rename columns\n",
    "df.rename(columns={'unique_id': \"id\",  # rename\n",
    "                   'name': \"name\",\n",
    "                   'strasse_nr': \"strasse_nr\",\n",
    "                   'bezirk': \"bezirk\",\n",
    "                   'lat': \"lat\",\n",
    "                   'lng': \"lng\",\n",
    "                   'lieferung': \"delivery\",\n",
    "                   'rating': \"rating\",\n",
    "                   'user_ratings_total': \"nr_ratings\",\n",
    "                   'price_level': \"price_lvl\",\n",
    "                   'types': \"types\"},\n",
    "          inplace=True)\n",
    "\n",
    "# Reorder columns\n",
    "df = df[['id', 'name', 'strasse_nr', 'bezirk', 'lat', 'lng', 'types', 'delivery', 'price_lvl', 'nr_ratings', 'rating']]\n",
    "\n",
    "# Remove duplicate information\n",
    "df['bezirk'] = df['bezirk'].str.replace(\"Bezirk \", \"\")\n",
    "\n",
    "# Set new index according to original csv file\n",
    "df.set_index('id', inplace=True)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>Unfortunately, some restaurants are missing entries. While that is fine for optional parameters like 'price_lvl', including places without any rating will bias the results. It is unlikely for a running restaurant business to have no Google rating. Therefore, one can assume, that either the API request wasn't succesful, the wrong output was matched or that the place is not a restaurant. For latter we can control by evaluating the 'types' parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty list\n",
    "removed_restaurants = []\n",
    "\n",
    "# Get index of restaurants with zero reviews\n",
    "idx_list = df[df.nr_ratings == 0].index\n",
    "idx_list2 = df[df.nr_ratings == None].index\n",
    "\n",
    "# Append removed restaurants to list\n",
    "for idx in idx_list:\n",
    "    removed_restaurants.append(df.loc[idx, 'name'])\n",
    "for idx in idx_list2:\n",
    "    removed_restaurants.append(df.loc[idx, 'name'])\n",
    "\n",
    "# Remove restaurants with zero ratings\n",
    "df.drop(df[df.nr_ratings == 0].index, inplace=True)\n",
    "df.drop(df[df.nr_ratings == None].index, inplace=True)\n",
    " \n",
    "# Remove and append restaurants with wrong type\n",
    "for idx, restaurant in df.iterrows():\n",
    "    types = restaurant['types']\n",
    "    try:\n",
    "        if any(type in \"restaurant and food and cafe and bakery and bar and meal_takeaway\" for type in types):  # substring search\n",
    "            pass\n",
    "        else:\n",
    "            df.drop(idx, axis=0, inplace=True)\n",
    "            removed_restaurants.append(restaurant['name'])\n",
    "    except TypeError:  # NaN = integer and needs to be dropped\n",
    "        df.drop(idx, axis=0, inplace=True)\n",
    "        removed_restaurants.append(restaurant['name'])\n",
    "\n",
    "# Remove duplicates\n",
    "list(set(removed_restaurants))\n",
    "           \n",
    "print(\"Number of restaurants removed: \" + str(len(removed_restaurants)))\n",
    "print(\"\\n\")\n",
    "print(removed_restaurants)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>As predicted some of the removed places are no restaurants (e.g. Foodlocker). There might have been a fault in the admission of the place to the dataset or other categories did not fit either. Delivery service platforms (e.g. abholservice24.de) and catering companies (e.g. Hefter Partyservice) are not supposed to exist in the \"Gastronomie\" category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, some places that are actual restaurants were removed. Most of them missed the Google rating within the API result. When googling \"Pizza Hof\" you'll see that there are 197 ratings with an average of 4.5 stars for that place. Unfortunately, no address is saved. Due to that and other reasons (i.e. moved/closed business, new opening), some restaurants with valid ratings had to be omitted from the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At last, the datatype of the columns is changed according to their values. For price level we take over Googles interpretation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change type of columns according to entries\n",
    "df[['lat', 'lng', 'rating']] = df[['lat', 'lng', 'rating']].apply(pd.to_numeric)  # float\n",
    "df[['nr_ratings']] = df[['nr_ratings']].astype(int)  # integer\n",
    "\n",
    "# Refactor price level values\n",
    "df.loc[df['price_lvl'] == 0, 'price_lvl'] = \"Free\"  # there should not be any entry\n",
    "df.loc[df['price_lvl'] == 1, 'price_lvl'] = \"Inexpensive\"\n",
    "df.loc[df['price_lvl'] == 2, 'price_lvl'] = \"Moderate\"\n",
    "df.loc[df['price_lvl'] == 3, 'price_lvl'] = \"Expensive\"\n",
    "df.loc[df['price_lvl'] == 4, 'price_lvl'] = \"Very Expensive\"\n",
    "df['price_lvl'] = df['price_lvl'].fillna(\"Unknown\")  # removes NaN\n",
    "\n",
    "# Refactor 'lieferservice' to boolean\n",
    "df.loc[df['delivery'] == \"WAHR\", 'delivery'] = 1\n",
    "df.loc[df['delivery'] == \"FALSCH\", 'delivery'] = 0\n",
    "df['delivery'] = df['delivery'].astype('bool')\n",
    "\n",
    "# Export modified dataframe for later use\n",
    "df.to_pickle(r'data/modif_restaurants.pkl')\n",
    "df.to_csv(r'data/modif_restaurants.csv', sep=';', encoding='utf-8', index=True)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving to more advanced research questions, the general composition of the Restaurants dataset shall be examined. Of special interest are the paramaters received through the Google API (i.e. 'price_lvl', 'rating', 'nr_ratings')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Visualization requires the following packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we removed incomplete rows and replaced missing values for 'price_lvl' with a string (i.e. \"Unknown\"), there should not be any NaN values left in our data. Pandas dataframes require columns to hold uniform data. Knowledge of the column type is important for later analyzes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create series with NaN count\n",
    "nan_series = df.isna().sum()\n",
    "\n",
    "# Create series with data type\n",
    "types_series = df.dtypes\n",
    "\n",
    "# Merge series to dataframe\n",
    "parameter_overview_df = pd.concat([nan_series, types_series], axis=1)\n",
    "parameter_overview_df.columns = ['NaN_count', 'dtype']\n",
    "                                  \n",
    "print(parameter_overview_df)\n",
    "print(\"\\n\")\n",
    "print(\"Shape of the dataframe: \" + str(df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is valid data for **674 restaurants** on **10 parameters** of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets have a closer look on the distribution of each relevant numeric parameter:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1) Delivery Service**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['delivery'].value_counts())\n",
    "\n",
    "# Series with frequency of Price Level\n",
    "delivery_distribution = df['delivery'].value_counts()\n",
    "\n",
    "# Creating bar plot\n",
    "X = delivery_distribution.index \n",
    "Y = delivery_distribution.values\n",
    "plt.pie(Y, labels=X, autopct='%.1f')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2) Price Level**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "print(df['price_lvl'].value_counts())\n",
    "\n",
    "# Series with frequency of Price Level\n",
    "price_lvl_distribution = df['price_lvl'].value_counts()\n",
    "price_lvl_distribution = price_lvl_distribution.reindex(index=['Unknown',  # ascending order\n",
    "                                                               'Inexpensive',\n",
    "                                                               'Moderate',\n",
    "                                                               'Expensive',\n",
    "                                                               'Very Expensive'])\n",
    "\n",
    "# Creating bar plot\n",
    "X = price_lvl_distribution.index \n",
    "Y = price_lvl_distribution.values\n",
    "plt.xlabel('price_lvl')\n",
    "plt.ylabel('frequency')\n",
    "plt.bar(X, Y, width=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most restaurants are in the \"Moderate\" price range. There are more \"Inexensive\" places than \"Expensive\" and \"Very Expensive\" ones. Google itself claims that the exact price range indicated by 'price_lvl' will vary from region to region. Therefore, one cannot specify any boundaries (e.g. 25€ to 35€ for a main course is expensive). Missing entries (i.e. \"Unknown\") might be caused due to a lack of reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3) Rating**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "# Series with frequency of Ratings\n",
    "rating_distribution = df['rating'].value_counts()\n",
    "rating_distribution = rating_distribution.sort_index(ascending=True)  # sort by value ascending\n",
    "\n",
    "# Statistical overview on parameter 'rating'\n",
    "describe_df = df['rating'].describe()\n",
    "rating_mean = describe_df.loc['mean']  # saving mean for later use\n",
    "\n",
    "print(describe_df.round(decimals=2))\n",
    "print(\"median: \" + str(df['rating'].median()))\n",
    "\n",
    "# Creating bar plot\n",
    "X = rating_distribution.index\n",
    "Y = rating_distribution.values\n",
    "plt.xlabel(\"rating\")\n",
    "plt.ylabel(\"frequency\")\n",
    "plt.bar(X, Y, width=0.3)\n",
    "plt.axvline(rating_mean, color='k', linestyle='dashed', linewidth=1)  # includes mean in plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though 'rating' is a discrete parameter, plotting its frequencies suggests a bell shaped distribution. While there are only a few places with extraordinary good (i.e. 4.75 to 5.0 stars) and bad (i.e. below 4.0 stars) ratings, the majority of restaurants have ratings between 4.0 and 4.75 stars. We try to examine the nature and possible bias of just this distribution later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4) Number of Ratings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "print(df['nr_ratings'].describe().round(decimals=1))\n",
    "print(\"median: \" + str(df['nr_ratings'].median()))\n",
    "\n",
    "# Creating bins for grouped frequencies\n",
    "bins_tpl = [(1, 100), (100, 200), (200, 300), (300, 400), (400, 500), (500, 600), (600, 700), (700, 800), (800, 900), (900, 1000), (1000, 2000), (2000, 15000)]\n",
    "bins = pd.IntervalIndex.from_tuples(bins_tpl)  # converts tuple to IntervalIndex, half-open by default\n",
    "cat_object = pd.cut(df['nr_ratings'], bins)  # creates categorial object\n",
    "\n",
    "# Statistical overview on bins\n",
    "nr_ratings_distribution = pd.value_counts(cat_object)\n",
    "nr_ratings_distribution = nr_ratings_distribution.sort_index(ascending=True)\n",
    "nr_ratings_distribution.index = nr_ratings_distribution.index.map(str)  # converts bins to strings and therefore categorial object to series\n",
    "\n",
    "# Creating bar plot\n",
    "X = nr_ratings_distribution.index\n",
    "y = nr_ratings_distribution.values\n",
    "fig, ax = plt.subplots()\n",
    "fig.autofmt_xdate()  # format x-axis label\n",
    "plt.xlabel(\"nr_ratings\")\n",
    "plt.ylabel(\"frequency\")\n",
    "plt.bar(X, y, width=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most places have been reviewed between 1 and 100 times. With ascending intervals, the frequency declines. Keep in mind that the last two intervals are defined on a larger range. Worth mentioning is the disparity between mean (447.3) and median (274)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of interest in our analyzes are the factors affecting a restaurants rating. Exposing a true causal effect is the desire of every Data Scientist. This however, is only possible under lab circumstances in an ideal randomized controlled experiment. \"Real world\" data has to be interpreted with care."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding the Restaurants data this means, that there might be a wide range of parameters that end up influencing the rating of a place. One cannot tell to what degree the price, food type or location end up affecting the rating, since there are a lot of other possible factors and biases. Therefore, one needs to settle for less. The aim is to find patterns in the data, indicating a correlation and a possible causal effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1) Rating and Number of Ratings**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This correlation indicates, whether restaurants with a lot of reviews tend to have better or worse ratings: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "# Simple linear regression of number of ratings on rating\n",
    "X = df['nr_ratings'].to_numpy()  # independent variable\n",
    "y = df['rating'].to_numpy()  # dependent variable\n",
    "X = X.reshape(-1, 1)\n",
    "y = y.reshape(-1, 1)\n",
    "linear_regressor = LinearRegression()  # create object for the class\n",
    "linear_regressor.fit(X, y)  # perform regression\n",
    "y_pred = linear_regressor.predict(X)  # make predictions\n",
    "\n",
    "# Create plot\n",
    "plt.scatter(X, y)\n",
    "plt.plot(X, y_pred, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression is a convenient tool for revealing correlations. When there is no data on some intervals, the regression line might be a good enough estimator but without any statement about parameter coherence. Therefore, the outlier with over 12000 ratings needs to be omitted. Additionally, the results shall be presented in an adequate way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "# Remove outlier\n",
    "small_restaurants_df = df.drop(df[df.nr_ratings > 5000].index, inplace=False) # drop restaurants with more then 5000 reviews\n",
    "\n",
    "# Simple linear regression of number of ratings on rating\n",
    "X = small_restaurants_df['nr_ratings'].to_numpy()  # independent variable\n",
    "y = small_restaurants_df['rating'].to_numpy()  # dependent variable\n",
    "X = X.reshape(-1, 1)\n",
    "y = y.reshape(-1, 1)\n",
    "linear_regressor = LinearRegression()  #create object for the class\n",
    "linear_regressor.fit(X, y)  # perform linear regression\n",
    "y_pred = linear_regressor.predict(X)  # make predictions\n",
    "\n",
    "# Create plot\n",
    "plt.scatter(X, y)\n",
    "plt.plot(X, y_pred, color='red')\n",
    "plt.legend(['regression line'])\n",
    "plt.xlabel('nr_ratings')\n",
    "plt.ylabel('rating')\n",
    "plt.show()\n",
    "\n",
    "print(\"Intercept: \\n\", linear_regressor.intercept_)\n",
    "print(\"Coefficient: \")\n",
    "for coefs in linear_regressor.coef_:\n",
    "    for coef in coefs:\n",
    "        print(format(coef, 'f'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regression line indicates a negative but weak correlation of 'nr_ratings' and 'rating'. An additional review will lower a restaurants rating on average by 0.000105 stars. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The robustness of this correlation is to doubt. There might be other factors that are correlated with the number of ratings which affect the rating (Omitted Variable Bias). What for example if the restaurant is part of a fast food chain? Then one could assume that such a place serves more daily customers and receives more ratings of which more are worse (due to lower food quality) than the average privately owned restaurant. We will reperform the regression and controll for latter in the next notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2) Rating, Number of Ratings and Price Level**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Including price level in the plot of 'rating' and 'nr_rating' might expose more information about their coherence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "# Scatterplot with Classification\n",
    "g = sns.scatterplot(\n",
    "    data=small_restaurants_df,\n",
    "    x=\"nr_ratings\", y=\"rating\", hue=\"price_lvl\"\n",
    "    )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When including price level the suggested correlation seems to hold. Additionally, we see that places without a price level (i.e. \"Unknown\"), tend to have less reviews. One could assume that this is about places that are newly openend or not that popular."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets have a closer look on the distribution of 'rating' and 'nr_ratings' within each category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe with group means\n",
    "means = df.groupby('price_lvl')[['rating', 'nr_ratings']].mean().round(decimals=2)\n",
    "means = means.reindex(index=['Unknown',  # reorder ascending\n",
    "                             'Inexpensive',\n",
    "                             'Moderate',\n",
    "                             'Expensive',\n",
    "                             'Very Expensive'\n",
    "                             ])\n",
    "\n",
    "means = means.rename(columns={'rating': \"mean_rating\", 'nr_ratings': \"mean_nr_ratings\"}) # rename\n",
    "\n",
    "# Create dataframe with group standard deviations\n",
    "stds = df.groupby('price_lvl')[['rating', 'nr_ratings']].std().round(decimals=2)\n",
    "stds = stds.rename(columns={'rating': \"std_rating\", 'nr_ratings': \"std_nr_ratings\"})\n",
    "\n",
    "# Merge dataframes to output dataframe\n",
    "oview = pd.concat([means, stds], axis=1)\n",
    "\n",
    "# Add mean and standard deviation of total sample as row to output dataframe\n",
    "describe_df = df.describe()\n",
    "mean_rating = describe_df.loc['mean', 'rating']\n",
    "mean_nr_ratings = describe_df.loc['mean', 'nr_ratings']\n",
    "std_rating = describe_df.loc['std', 'rating']\n",
    "std_nr_ratings = describe_df.loc['std', 'nr_ratings']\n",
    "\n",
    "# Count frequency of each row\n",
    "counter_total = describe_df.loc['count', 'rating']\n",
    "counter_series = df['price_lvl'].value_counts()\n",
    "\n",
    "# Append series as column to output dataframe\n",
    "oview['freq'] = counter_series\n",
    "oview.loc['Total'] = [mean_rating, mean_nr_ratings, std_rating, std_nr_ratings, counter_total]\n",
    "\n",
    "# Format output dataframe\n",
    "oview = oview.astype({'freq': int})\n",
    "oview = oview.round(decimals=2)\n",
    "\n",
    "print(oview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>Worth mentioning is the above average standard deviation of ratings within the \"Unknown\" price category. This might be related to the fact, that not yet categorized restaurants tend to have less reviews. Since users can only give one, two, three, four or five stars the variance is on average higher when there are not many reviews. Indeed, the mean number of ratings for uncategorized is far below the sample average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also moticable is that restaurants within the \"Moderate\" and \"Very Expensive\" price level have been rated more often with ratings below the sample average. While for \"Very Expensive\" the sample size is nowhere near sufficient large, the \"Moderate\" priced restaurants can be acknowledged as representative. Surprisingly, most FastFood cooperations (e.g. McDonald's, Subway) are categorized as \"Moderate\" expensive by Google."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
